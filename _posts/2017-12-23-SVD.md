---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---

## Definitions



## PCA's objective corresponds with SVD

Given any matrix \\(X\\), it is factorized into \\(U \Sigma V^T \\) 

$$ X = U \Sigma V^T $$

SVD answers 2 important questions asked by PCA:
1. If we have to cut 1 axis, which one should we cut that minimize the lost of information? Alternatively, what transformation matrix or new coordiante system best approximates the data? (change of basis)
2. After finding the new coordinate system, what is the distance bewteen all data points?

Essentially the following formula answers both:

$$ X = U\Sigma V^T $$

$$ XV = U\Sigma $$

where \\(V\\) is change of basis matrix, \\(U\\) is the data distance on new coordinate system and \\(\Sigma\\) scale the data distance with variance in new coordinate system axis. 


There is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/). (Basically you could drag the coordinate system around to see which angle has the most variance)



Lets see how does computing SVD answer these 2 questions!


















## Which axis should we cut: change of basis matrix

The answer might not be that clear in original coordinate system. 


Assuming Before change of basis blue arrow is x-axis, red y-axis, green z-axis.
<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis
</div>
</div>

<div class="imgcap">
<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>


But if we change the basis by rotating the coordinate system, blue arrow arguably should be the one that gets cut as it has the least variance within the new coordinate system. (notice blue will no longer represents the original x-axis)


There are 2 steps in computing SVD:

1. solve \\(X^T X\\)

2. solve \\(XV = U\Sigma\\)


\\(X^T X\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.)

$$ 
\begin{align}
X &= U \Sigma V^T \\

X^T X &= {(U\Sigma V^T)}^T (U \Sigma V^T) \\
X^T X &= {(V\Sigma U^T)} (U \Sigma V^T)\\
& \text{orthonormal eigenvectors transpose multiplication results in identity matrix} &\\

X^T X &= V {\Sigma}^2 V^T 

\end{align}
$$

where \\(V {\Sigma}^2 V^T \\) is the diagonalization of \\(X^T X\\) 







Lets talk about 2 popular ways of solving \\( X^T X = V {\Sigma}^2 V^T\\)

1. solving determinant = 0 to find eigenvalue

$$
\begin{align}

X^T X &= V {\Sigma}^2 V^T \\
X^T X V &= {\Sigma}^2 V \\
(X^T X - {\Sigma}^2) V &= 0\\
\det(X^T- \Sigma^2 I) &= 0

\end{align}

$$

Geometrically, this corresponds to rotating transformation matrix (i.e. \\X^T X - {\Sigma}^2\\)) until an angle that squishes eigenvector \\(V\\)to 0.

<div class="imgcap">
<img src="/assets/svd/squish1.png" height="200" style="float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/squish2.png" height="200" style="float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
</div>
</div>

A [good visualization](http://www.youtube.com/watch?v=PFDu9oVAE-g&t=7m30s) is here.



It might not seem as obvious as why we compute covariance matrix other than mathematically it cancels out U. 2nd method intuitively shows computing covariance matrix is directly computing the new coordinate system.

2. random surfer power iteration 

[pagerank random surfer](www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html)

Essentially how much weights each attributes should be given dynamic system of interaction between attributes as denoted in covariance matrix.


Lets use the real data to solidify our understanding!

Here is some fake data from that approximates [SVD visualization graph](http://setosa.io/ev/principal-component-analysis/)'s dataset. It is clear that there is a almost upward trend of the data.


Assuming matrix \\( X \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. 


```python
import numpy as np
from numpy.linalg import svd

X =  np.array([
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
])

U, Sigma, V_transpose = svd(X, full_matrices=False)
Covariance_matrix = np.dot(dataset.T, dataset)
print(Covariance_matrix, U,singularValues,V_transpose)
```

output:
```python

## Covariance matrix between attributes, x, y, z
[[ 204.  204.  102.]
 [ 204.  204.  102.]
 [ 102.  102.   51.]]

## U, reduced, not full-rank(full rank just pad number to artificially creates orthornormal square matrix)
[[-0.070014   -0.97259152  0.02717264]
 [-0.14002801  0.04421983  0.98874033]
 [-0.21004201 -0.01941955 -0.02452013]
 [-0.28005602  0.08843966 -0.05259622]
 [-0.35007002  0.02480028 -0.05081823]
 [-0.42008403 -0.0388391  -0.04904025]
 [-0.49009803 -0.10247848 -0.04726227]
 [-0.56011203  0.17687931 -0.10519243]]

## Sigma
[  2.14242853e+01   1.93007163e-15   2.53327545e-31]

## V_tranpose, othornormal 
[[-0.66666667 -0.66666667 -0.33333333]
 [ 0.74535599 -0.59628479 -0.2981424 ]
 [-0.          0.4472136  -0.89442719]]

```
[-0.66666667 -0.66666667 -0.33333333] is the new angle (column vector) that results in most variance. Notice that (x,y) **co-varies** 2 times than (y,z) or (x,z) in covariance matrix, which coincides with the 1st new angle in SVD.

After computing the new change of basis matrix, we could answer the **2nd question**.

























## Computing distance

Given we have found the new coordinate system, how to find the distance of each data point on the new coordinate system?

Recall there are 2 steps in computing SVD

1. solve \\(X^T X = V\Sigma^2 V^T\\)
2. solve \\(XV = U\Sigma\\)


Recall that \\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\Sigma V^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.


Computing \\(XV\\) is actually computing the distance between the new coordinate system and data points. We need our friend projection onto subspace.

Here is a visual picture to see this:

<div class="imgcap">
<img src="/assets/svd/distance.png" height="200">
<div class="thecap"> computing distance between data point and new axis
</div>
</div>











$$ (U\Sigma V^T)^T $$





\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)