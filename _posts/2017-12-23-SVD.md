---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---

## PCA's objective corresponds with SVD

Given any matrix \\(X\\), it is factorized into \\(U \Sigma V^T \\) 

$$ X = U \Sigma V^T $$

SVD answers 2 important questions asked by PCA:
1. If we have to cut 1 axis, which one should we cut that minimize the lost of information? Alternatively, what transformation matrix or new coordiante system best approximates the data? (change of basis)
2. After finding the new coordinate system, what is the distance bewteen all data points?

Essentially the following formula answers both:

$$ X = U\Sigma V^T $$

$$ XV = U\Sigma $$

where \\(V\\) is change of basis matrix, \\(U\\) is the data distance on new coordinate system and \\(\Sigma\\) scale the data distance with variance in new coordinate system axis. 


There is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/). (Basically you could drag the coordinate system around to see which angle has the most variance)



Lets see how does computing SVD answer these 2 questions!


















## 1st question: Which axis should we cut: change of basis matrix

The answer might not be that clear in original coordinate system. 


Assuming Before change of basis blue arrow is x-axis, red y-axis, green z-axis.
<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis
</div>
</div>

<div class="imgcap">
<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>


But if we change the basis by rotating the coordinate system, blue arrow arguably should be the one that gets cut as it has the least variance within the new coordinate system. (notice blue will no longer represents the original x-axis)


**Example code:**

Lets use the real data to solidify our understanding of change of basis!

Here is some fake data from that approximates [SVD visualization graph](http://setosa.io/ev/principal-component-analysis/)'s dataset. It is clear that there is a almost upward trend of the data. Assuming matrix \\( X \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. 


```python
import numpy as np
from numpy.linalg import svd

X =  np.array([
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
])

U, Sigma, V_transpose = svd(X, full_matrices=False)
Covariance_matrix = np.dot(dataset.T, dataset)
print(Covariance_matrix, U,singularValues,V_transpose)
```

output:
```python

## Covariance matrix between attributes, x, y, z
[[ 204.  204.  102.]
 [ 204.  204.  102.]
 [ 102.  102.   51.]]

## U, reduced, not full-rank(full rank just pad number to artificially creates orthornormal square matrix)
[[-0.070014   -0.97259152  0.02717264]
 [-0.14002801  0.04421983  0.98874033]
 [-0.21004201 -0.01941955 -0.02452013]
 [-0.28005602  0.08843966 -0.05259622]
 [-0.35007002  0.02480028 -0.05081823]
 [-0.42008403 -0.0388391  -0.04904025]
 [-0.49009803 -0.10247848 -0.04726227]
 [-0.56011203  0.17687931 -0.10519243]]

## Sigma
[  2.14242853e+01   1.93007163e-15   2.53327545e-31]

## V_tranpose, othornormal 
[[-0.66666667 -0.66666667 -0.33333333]
 [ 0.74535599 -0.59628479 -0.2981424 ]
 [-0.          0.4472136  -0.89442719]]

```
[-0.66666667 -0.66666667 -0.33333333] is the new angle (column vector) that results in most variance. Notice that (x,y) **co-varies** 2 times than (y,z) or (x,z) in covariance matrix, which coincides with the 1st new angle in SVD.



**There are 2 steps in computing SVD:**

1. solve \\(X^T X\\)

2. solve \\(XV = U\Sigma\\)


\\(X^T X\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.)

$$ 
\begin{align}
X &= U \Sigma V^T \\

X^T X &= {(U\Sigma V^T)}^T (U \Sigma V^T) \\
X^T X &= {(V\Sigma U^T)} (U \Sigma V^T)\\
& \text{orthonormal eigenvectors transpose multiplication results in identity matrix} &\\

X^T X &= V {\Sigma}^2 V^T 

\end{align}
$$

where \\(V {\Sigma}^2 V^T \\) is the diagonalization of \\(X^T X\\) 





It might not seem straightly obvious as why we compute covariance matrix other than mathematically it cancels out U. Here are 2 ways to see intuitively why computing covariance matrix (i.e. \\( X^T X = V {\Sigma}^2 V^T\\)) is directly computing the new coordinate system that maximizes variance.


**1. solving determinant = 0 to find eigenvalue**

$$
\begin{align}

X^T X &= V {\Sigma}^2 V^T \\
X^T X V &= {\Sigma}^2 V \\
(X^T X - {\Sigma}^2 I) V &= 0\\
\det(X^T X- \Sigma^2 I) &= 0

\end{align}

$$


\\(X^T X = V {\Sigma}^2 V^T\\) is diagonalization. Gemetrically, it corresponds to rotate, stretches and rotates back and variance is maximized in that new coordinate system.

<div class="imgcap">
<img src="/assets/svd/diagonalization.png" height="200">
</div>

But why does rotating covariance matrix (diagonalization) implies \\(\argmax_\\) variance in rotating tranformation matrix (\\(XV = U\Sigma\\))? (try to picture 2 rotations simultaneously side by side)


Focusing on \\(\det(X^T X- \Sigma^2 I) = 0\\), rotating the **covariance matrix** (notice not *data matrix*) is maximizing the variance (i.e. \\(\Sigma\\)) in new coordinate system under constraint. 

The contraint is linear dependence. Each column vector in covaraicne matrix could only move along in 1 axis (recall \\(\det(X^T X- \Sigma^2 I) = 0\\)) until rotation squishes eigenvector \\(V\\)to 0.

Lambda is contrained for there to be a solution of eigenvector.

A [good visualization](http://www.youtube.com/watch?v=PFDu9oVAE-g&t=7m30s)of such constraint in rotating covaraince matrix is here:

<div class="imgcap">
<img src="/assets/svd/squish1.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/squish2.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/squish3.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">
</div>
</div>
</div>

(note: this above example is not a symmetric matrix like covariance matrix)






It does not matter what the shape of data looks like there are always 3 values of lambda that 

big coorelation i.e. similar data, less lambda i.e. less rotate to being together

less lambda mean less covariance

similar col space => either no need to change or 180 degree changes
bigger difference between the biggest lambda and smallest lambda


recall visualization graph => one more step to convert to convariance matrix column vectors



how to rotate so linear dependent => each col vector could only rotate in 1 direction


linear dependence == direction of maximum stretch + maximizing variance

in the visualization graph director vector is rotating as well to give such lambda











**2. Solve for eigenvector instead! It is already implied in covaraicne matrix**

If you understand how to computing pagerank, this should be rather straightforward.


Covaraince matrix diagonalization could be viewed as:

<div class="imgcap">
<img src="/assets/svd/page2.png" height="200" style="float: left; width: 49%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page1.png" height="200" style="float: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;">

</div>
</div>

where each node represents an attribute (i.e. x,y,z) and each edge denotes a covariance entry between 2 attributes.

Recall:

$$X^T X V = {\Sigma}^2 V$$


Solving \\(V\\) is essentially solving how much weights should be given to each attributes given the dynamic system of interaction between attributes as denoted in covariance matrix. The resulting weights vector is eigenvector for change of basis.

To [compute with power iteration until it converges](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html), start with random vector \\(v\\): 

<div class="imgcap">
<img src="/assets/svd/page3.png" height="200" >

</div>

Iterating \\(v, Av, ..., A^k v\\) converges to equilibrium value of weights vector. 

For a random surfer perspective, we can model the process as a random walk on graphs with edges as probability ended up on outgoing node. \\(v, Av, ..., A^k v\\) converges to final probabilitic vector.

Both weights vector or probabilistic vector denotes eigenvector. It concludes how important that attribute is and hence how much to rotate in stationary distribution.

Switching from probabilitic or dynamic system point of view to just linear algebra, this eigenvector could be directly computed by solving \\(V\\) in \\(X^T X V = {\Sigma}^2 V\\):


<div class="imgcap">
<img src="/assets/svd/page4.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page5.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page6.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">
</div>
</div>
</div>





After computing the new change of basis matrix, we could answer the **2nd question**.

























## 2nd question: given new change of basis, how to compute distance of each data point on the new coordinate system?

Recall there are 2 steps in computing SVD:

1. solve \\(X^T X = V\Sigma^2 V^T\\)
2. solve \\(XV = U\Sigma\\)

1st step already does most of the heavy lifting. 2nd step is rather strightforward: solve \\(U\\) with \\(V\\) and \\(\Sigma \\) from step 1. The interesting part of 2nd step is geometrically computing \\(XV\\) is actually computing the distance between the new coordinate system and data points. To see this, we need to look at projection of \\(X\\) onto subspace \\(V\\) by taking dot products \\(XV\\).


Here is a visual picture of computing distance between data points and new axis:

<div class="imgcap">
<img src="/assets/svd/distance.png" height="200">
</div>


\\( proj_v(x)\\) linear maps input vector \\(x\\) and output projection of \\(x\\) onto \\(v\\) where \\( proj_v(x): \mathbb{R^n} \to \mathbb{R^n} \\) 

squared length of projections is computed by taking dot product: 
\\( |x \cdot y| \\)

\\( proj_v(x)\\): v scaled by inner product of x and v. 

\\(XV\\) is projection length of all data points \\(x_i\\) onto 3 new coordinate lines spanned by \\(V\\). Each entry is dot products \\( x_i \cdot v_i \\). The sqaured norm \\({|XV|}^2\\) denotes sum of squared lengths of projections of data onto line spanned by \\(v\\)

$$
\begin{align}
proj_v(x) &= (x\cdot v) \cdot v \\

\sum_{i} {x_i}^2 &= (proj_v(x))^2 + (dist_v(x))^2 \\

\sum_{i} {x_i}^2 - (dist_v(x))^2 &= (proj_v(x))^2 \\

(dist_v(x))^2 &= (\sum_{i=1}^{n} {x_i}^2) - |x \cdot v|
\end{align}
$$

\\(dist_v(x)\\) distance of \\(x \\) from line spanned by unit vector \\(v\\). Given x is constant, **maximizing sum of squared lengths of projections is the same as minimizing squared Euclidean distance between projected line and data points**


Formally:

$$ \DeclareMathOperator*{\argmax}{arg\,max} \argmax_{||v||=1} \frac{1}{m} \displaystyle \sum_{i=1}^{m} {\langle x_i, v \rangle}^2 $$


$$ \DeclareMathOperator*{\argmax}{arg\,max} \argmax_{v:||v||=1} |Xv|$$

$$ \DeclareMathOperator*{\argmax}{arg\,max} v_2 = \argmax_{v \bot v_1, ||v|| =1} |Xv| $$



## SVD from scratch with power iteration

```python

import numpy as np
from numpy.linalg import norm
 
from random import normalvariate
from math import sqrt
 
def randomUnitVector(n):
    unnormalized = [normalvariate(0, 1) for _ in range(n)]
    theNorm = sqrt(sum(x * x for x in unnormalized))
    return [x / theNorm for x in unnormalized]
 
def svd_1d(A, epsilon=1e-10):
    ''' The one-dimensional SVD '''
 
    n, m = A.shape
    x = randomUnitVector(m)
    lastV = None
    currentV = x
    B = np.dot(A.T, A)
 
    iterations = 0
    while True:
        iterations += 1
        lastV = currentV
        currentV = np.dot(B, lastV)
        currentV = currentV / norm(currentV)
 
        if abs(np.dot(currentV, lastV)) > 1 - epsilon:
            print("converged in {} iterations!".format(iterations))
            return currentV

def svd(A, epsilon=1e-10):
    n, m = A.shape
    svdSoFar = []
 
    for i in range(m):
        matrixFor1D = A.copy()
 
        for singularValue, u, v in svdSoFar[:i]:
            matrixFor1D -= singularValue * np.outer(u, v)
 
        v = svd_1d(matrixFor1D, epsilon=epsilon)  # next singular vector
        u_unnormalized = np.dot(A, v)
        sigma = norm(u_unnormalized)  # next singular value
        u = u_unnormalized / sigma
 
        svdSoFar.append((sigma, u, v))
 
    # transform it into matrices of the right shape
    singularValues, us, vs = [np.array(x) for x in zip(*svdSoFar)]
 
    return singularValues, us.T, vs            

if __name__ == "__main__":
    movieRatings = np.array([
        [2, 5, 3],
        [1, 2, 1],
        [4, 1, 1],
        [3, 5, 2],
        [5, 3, 1],
        [4, 5, 5],
        [2, 4, 2],
        [2, 2, 5],
    ], dtype='float64')

theSVD = svd(movieRatings)
print(theSVD[0], theSVD[1], theSVD[2])    

```


<div class="imgcap">
<img src="/assets/svd/distance2.png" height="200">
</div>


## Reference and credits:



## Appendix:

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\Sigma V^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.


$$
\begin{align}

\DeclareMathOperator*{\argmin}{arg\,min}\\
\DeclareMathOperator*{\argmax}{arg\,max}\\


\argmax_{||v||=1} \frac{1}{m} \displaystyle \sum_{i=1}^{m} {\langle x_i, v \rangle}^2 \\

v^T X^T Xv = (Xv)^T (Xv) \\

\argmax_{||v||=1} v^T Av


\end{align}

$$
