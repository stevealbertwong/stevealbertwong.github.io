---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---



## PCA's objective corresponds with SVD

Given any matrix \\(X\\), it is factorized into \\(U \Sigma V^T \\) 

$$ X = U \Sigma V^T $$

SVD answers 2 important questions asked by PCA:
1. If we have to cut 1 axis, which one should we cut that minimize the lost of information? Alternatively, what transformation matrix or new coordiante system best approximates the data? (change of basis)
2. After finding the new coordinate system, what is the distance bewteen all data points?

Essentially the following formula answers both:

$$ X = U\Sigma V^T $$
$$ U^T X = \Sigma V^T $$
$$ XV = \Sigma U $$
$$ (XV)^T = (\Sigma U)^T $$
$$ V^T X^T = U^T /Sigma $$ 

where \\(V^T\\) is change of basis matrix, \\(U^T\\) is the data distance on new coordinate system and \\(\Sigma\\) scale the data distance with variance in new coordinate system axis. 


This is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/).

<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis



<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>
</div>

> Assuming blue arrow is x-axis, red y-axis, green z-axis.

Here is some fake data from that represents that graph's dataset. It is clear that there is a almost upward trend of the data.

```python
import numpy as np

# each row represents 1 data point, with 3 attributes x,y,z
dataset = [
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
]
```

Lets see how does computing SVD answer these 2 questions!





**Which axis should we cut: change of basis matrix**

Which axis should we cut? The answer might not be that clear in before change of basis. 

But if we change the basis by rotating the coordinate system, blue arrow arguably should be the one that gets cut as it has the least variance within the new coordinate system. (notice blue is no longer x-axis)


Geometrically, SVD could be think of a rotation, stretch and rotation. 

$$ A = U \Sigma V^T $$

$$ U^T A = \Sigma V^T $$




Assuming matrix \\( A \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. \\(A^T A\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.



$$ 
\begin{align}
A = U \Sigma V^T 
\end{align}
$$

$$ A^T A = {(U\Sigma V^T)}^T (U \Sigma V^T) $$

$$ A^T A = {(V\Sigma U^T)} (U \Sigma V^T) $$

$$ A^T A = V {\Sigma}^2 V^T $$ 

> note: since orthonormal eigenvectors transpose multiplication results in identity matrix


\\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\Sigma V^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.


Take a look at my fake data's SVD:

```python
import numpy as np
from numpy.linalg import svd

dataset =  np.array([
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
])

U, Sigma, V_transpose = svd(dataset, full_matrices=False)
Covariance_matrix = np.dot(dataset.T, dataset)
print(Covariance_matrix, U,singularValues,V_transpose)
```

output:
```python

## Covariance matrix between attributes, x, y, z
[[ 204.  204.  102.]
 [ 204.  204.  102.]
 [ 102.  102.   51.]]

## U, reduced, not full-rank(full rank just pad number to artificially creates orthornormal square matrix)
[[-0.070014   -0.97259152  0.02717264]
 [-0.14002801  0.04421983  0.98874033]
 [-0.21004201 -0.01941955 -0.02452013]
 [-0.28005602  0.08843966 -0.05259622]
 [-0.35007002  0.02480028 -0.05081823]
 [-0.42008403 -0.0388391  -0.04904025]
 [-0.49009803 -0.10247848 -0.04726227]
 [-0.56011203  0.17687931 -0.10519243]]

## Sigma
[  2.14242853e+01   1.93007163e-15   2.53327545e-31]

## V_tranpose, othornormal 
[[-0.66666667 -0.66666667 -0.33333333]
 [ 0.74535599 -0.59628479 -0.2981424 ]
 [-0.          0.4472136  -0.89442719]]

```
[-0.66666667 -0.66666667 -0.33333333] is the new angle that provides most variance. One interesting thing to notice is that (x,y) **co-varies** 2 times than (y,z) or (x,z) in covariance matrix, which coincides with the 1st new angle in SVD.

Intuitively, we have already computed the angle with the most variance when computing covariance matrix in 1st step of SVD.

After computing the new change of basis matrix, we could answer the **2nd question**


**SVD from scratch: Power iteration to compute**


$$ (U\Sigma V^T)^T $$





\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)