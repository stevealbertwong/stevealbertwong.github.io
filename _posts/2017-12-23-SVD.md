---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---



** SVD objective **

Given any matrix \\(A\\), it is factorized into \\(U \Sigma V^T \\) 

$$ A = U \Sigma V^T $$






** Change of basis **

Geometrically, SVD could be think of a rotation, stretch and rotation. 

$$ A = U \Sigma V^T $$
$$ U^T $$



Assuming matrix \\(A\\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. \\(A^T A\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.

$$ A = U \Sigma V^T $$
$$ A^T A = {(U\Sigma V^T)}^T (U \Sigma T^T) $$

$$ A^T A = {(V\Sigma U^T)} (U \Sigma T^T) $$
$$ A^T A = U {\Sigma}^2 U $$ 
since orthonormal eigenvectors transpose mulitply by itself results in identity matrix


$$ (U\Sigma V^T)^T $$



** Covariance matrix **

\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)