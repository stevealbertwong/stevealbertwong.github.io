---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---

## PCA's objective corresponds with SVD

Given any matrix \\(X\\), it is factorized into \\(U \Sigma V^T \\) 

$$ X = U \Sigma V^T $$

SVD answers 2 important questions asked by PCA:
1. If we have to cut 1 axis, which one should we cut that minimize the lost of information? Alternatively, what transformation matrix or new coordiante system best approximates the data? (change of basis)
2. After finding the new coordinate system, what is the distance bewteen all data points?

Essentially the following formula answers both:

$$ X = U\Sigma V^T $$

$$ XV = U\Sigma $$

where \\(V\\) is change of basis matrix, \\(U\\) is the data distance on new coordinate system and \\(\Sigma\\) scale the data distance with variance in new coordinate system axis. 


There is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/). (Basically you could drag the coordinate system around to see which angle has the most variance)



Lets see how does computing SVD answer these 2 questions!


















## Which axis should we cut: change of basis matrix

The answer might not be that clear in original coordinate system. 


Assuming Before change of basis blue arrow is x-axis, red y-axis, green z-axis.
<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis
</div>
</div>

<div class="imgcap">
<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>


But if we change the basis by rotating the coordinate system, blue arrow arguably should be the one that gets cut as it has the least variance within the new coordinate system. (notice blue will no longer represents the original x-axis)


**Example code:**

Lets use the real data to solidify our understanding of change of basis!

Here is some fake data from that approximates [SVD visualization graph](http://setosa.io/ev/principal-component-analysis/)'s dataset. It is clear that there is a almost upward trend of the data. Assuming matrix \\( X \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. 


```python
import numpy as np
from numpy.linalg import svd

X =  np.array([
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
])

U, Sigma, V_transpose = svd(X, full_matrices=False)
Covariance_matrix = np.dot(dataset.T, dataset)
print(Covariance_matrix, U,singularValues,V_transpose)
```

output:
```python

## Covariance matrix between attributes, x, y, z
[[ 204.  204.  102.]
 [ 204.  204.  102.]
 [ 102.  102.   51.]]

## U, reduced, not full-rank(full rank just pad number to artificially creates orthornormal square matrix)
[[-0.070014   -0.97259152  0.02717264]
 [-0.14002801  0.04421983  0.98874033]
 [-0.21004201 -0.01941955 -0.02452013]
 [-0.28005602  0.08843966 -0.05259622]
 [-0.35007002  0.02480028 -0.05081823]
 [-0.42008403 -0.0388391  -0.04904025]
 [-0.49009803 -0.10247848 -0.04726227]
 [-0.56011203  0.17687931 -0.10519243]]

## Sigma
[  2.14242853e+01   1.93007163e-15   2.53327545e-31]

## V_tranpose, othornormal 
[[-0.66666667 -0.66666667 -0.33333333]
 [ 0.74535599 -0.59628479 -0.2981424 ]
 [-0.          0.4472136  -0.89442719]]

```
[-0.66666667 -0.66666667 -0.33333333] is the new angle (column vector) that results in most variance. Notice that (x,y) **co-varies** 2 times than (y,z) or (x,z) in covariance matrix, which coincides with the 1st new angle in SVD.



**There are 2 steps in computing SVD:**

1. solve \\(X^T X\\)

2. solve \\(XV = U\Sigma\\)


\\(X^T X\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.)

$$ 
\begin{align}
X &= U \Sigma V^T \\

X^T X &= {(U\Sigma V^T)}^T (U \Sigma V^T) \\
X^T X &= {(V\Sigma U^T)} (U \Sigma V^T)\\
& \text{orthonormal eigenvectors transpose multiplication results in identity matrix} &\\

X^T X &= V {\Sigma}^2 V^T 

\end{align}
$$

where \\(V {\Sigma}^2 V^T \\) is the diagonalization of \\(X^T X\\) 





It might not seem straightly obvious as why we compute covariance matrix other than mathematically it cancels out U. Here are 2 ways to see intuitively why computing covariance matrix (i.e. \\( X^T X = V {\Sigma}^2 V^T\\)) is directly computing the new coordinate system that maximizes variance.


**1. solving determinant = 0 to find eigenvalue**

$$
\begin{align}

X^T X &= V {\Sigma}^2 V^T \\
X^T X V &= {\Sigma}^2 V \\
(X^T X - {\Sigma}^2 I) V &= 0\\
\det(X^T X- \Sigma^2 I) &= 0

\end{align}

$$


\\(X^T X = V {\Sigma}^2 V^T\\) is diagonalization. Gemetrically, it corresponds to rotate, stretches and rotates back and variance is maximized in that new coordinate system.

<div class="imgcap">
<img src="/assets/svd/diagonalization.png" height="200">
</div>

But why does rotating covariance matrix (diagonalization) implies \\(\argmax_\\) variance in rotating tranformation matrix (\\(XV = U\Sigma\\))? (try to picture 2 rotations simultaneously side by side)


Focusing on \\(\det(X^T X- \Sigma^2 I) = 0\\), rotating the **covariance matrix** (notice not *data matrix*) is maximizing the variance (i.e. \\(\Sigma\\)) in new coordinate system under constraint. 

The contraint is linear dependence. Each column vector in covaraicne matrix could only move along in 1 axis (recall \\(\det(X^T X- \Sigma^2 I) = 0\\)) until rotation squishes eigenvector \\(V\\)to 0.

Lambda is contrained for there to be a solution of eigenvector.

A [good visualization](http://www.youtube.com/watch?v=PFDu9oVAE-g&t=7m30s)of such constraint in rotating covaraince matrix is here:

<div class="imgcap">
<img src="/assets/svd/squish1.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/squish2.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/squish3.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">
</div>
</div>
</div>

(note: this above example is not a symmetric matrix like covariance matrix)






It does not matter what the shape of data looks like there are always 3 values of lambda that 

big coorelation i.e. similar data, less lambda i.e. less rotate to being together

less lambda mean less covariance

similar col space => either no need to change or 180 degree changes
bigger difference between the biggest lambda and smallest lambda


recall visualization graph => one more step to convert to convariance matrix column vectors



how to rotate so linear dependent => each col vector could only rotate in 1 direction


linear dependence == direction of maximum stretch + maximizing variance

in the visualization graph director vector is rotating as well to give such lambda











**2. Solve for eigenvector instead! It is already implied in covaraicne matrix**

If you understand how to computing pagerank, this should be rather straightforward.


Covaraince matrix diagonalization could be viewed as:

<div class="imgcap">
<img src="/assets/svd/page2.png" height="200" style="float: left; width: 49%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page1.png" height="200" style="float: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;">

</div>
</div>

where each node represents an attribute (i.e. x,y,z) and each edge denotes a covariance entry between 2 attributes.

Recall:

$$X^T X V = {\Sigma}^2 V$$


Solving \\(V\\) is essentially solving how much weights should be given to each attributes given the dynamic system of interaction between attributes as denoted in covariance matrix. The resulting weights vector is eigenvector for change of basis.

To [compute with power iteration until it converges](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html), start with random vector \\(v\\): 

<div class="imgcap">
<img src="/assets/svd/page3.png" height="200" >

</div>

Iterating \\(v, Av, ..., A^k v\\) converges to equilibrium value of weights vector (eigenvector). 

For a random surfer perspective, we can model the process as a random walk on graphs with edges as probability ended up on outgoing node. \\(v, Av, ..., A^k v\\) converges to final probabilitic vector.

Both weights vector or probabilistic vector denotes eigenvector. It concludes how important that attribute is and hence how much to rotate in stationary distribution.

Switching from probabilitic or dynamic system point of view, this eigenvector could be directly solved by \\(X^T X V = {\Sigma}^2 V\\):


<div class="imgcap">
<img src="/assets/svd/page4.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page5.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">

<div class="imgcap">
<img src="/assets/svd/page6.png" height="200" style="float: left; width: 32%; margin-right: 1%; margin-bottom: 0.5em;">
</div>
</div>
</div>





After computing the new change of basis matrix, we could answer the **2nd question**.

























## Computing distance

Given we have found the new coordinate system, how to find the distance of each data point on the new coordinate system?

Recall there are 2 steps in computing SVD

1. solve \\(X^T X = V\Sigma^2 V^T\\)
2. solve \\(XV = U\Sigma\\)


Recall that \\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\Sigma V^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.


Computing \\(XV\\) is actually computing the distance between the new coordinate system and data points. We need our friend projection onto subspace.

Here is a visual picture to see this:

<div class="imgcap">
<img src="/assets/svd/distance.png" height="200">
<div class="thecap"> computing distance between data point and new axis
</div>
</div>


$$
\begin{align}

\DeclareMathOperator*{\argmin}{arg\,min}\\
\DeclareMathOperator*{\argmax}{arg\,max}\\


\argmax_{||v||=1} \frac{1}{m} \displaystyle \sum_{i=1}^{m} {\langle x_i, v \rangle}^2 \\

v^T X^T Xv = (Xv)^T (Xv) \\

\argmax_{||v||=1} v^T Av


\end{align}

$$


where m is the number of datapoints


\\(XV\\) is projection length of all data points \\(x_i\\) onto 3 new coordinate lines spanned by \\(V\\)




## Latent Sementic Analysis

Reference and credits:
