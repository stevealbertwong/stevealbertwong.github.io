---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---



## PCA's objective corresponds with SVD

Given any matrix \\(A\\), it is factorized into \\(U \Sigma V^T \\) 

$$ A = U \Sigma V^T $$



This is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/).

<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis



<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>
</div>

> Assumes blue array is x-axis, red y-axis, green z-axis.

Here is some fake data from that represents that graph's dataset. It is clear that there is a almost upward trend of the data.



```python
import numpy as np

# each row represents 1 data point, with 3 attributes x,y,z
dataset = [
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
]
```


The real question PCA is asking is if we have to cut 1 axis, which one should we cut that minimize the lost of information. The answer might not be that clear in before change of basis. But if we change the basis by rotating the coordinate system, blue arguably should be the one gets cut as it has the least variance within the new coordinate system.(notice blue is no longer x-axis)



**Covariance matrix**

Geometrically, SVD could be think of a rotation, stretch and rotation. 

$$ A = U \Sigma V^T $$

$$ U^T A = \Sigma V^T $$




Assuming matrix \\( A \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. \\(A^T A\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.



$$ 
\begin{align}
A = U \Sigma V^T 
\end{align}
$$

$$ A^T A = {(U\Sigma V^T)}^T (U \Sigma V^T) $$

$$ A^T A = {(V\Sigma U^T)} (U \Sigma V^T) $$

$$ A^T A = V {\Sigma}^2 V^T $$ 

> note: since orthonormal eigenvectors transpose multiplication results in identity matrix


\\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\Sigma V^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.


Take a look at my fake data's SVD:

```python
import numpy as np
from numpy.linalg import svd

dataset =  np.array([
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
])

U, Sigma, V_transpose = svd(dataset)
Covariance_matrix = np.dot(dataset.T, dataset)
print(Covariance_matrix, U,singularValues,V_transpose)
```

output:
```python

## Covariance matrix between attributes, x, y, z
[[ 204.  204.  102.]
 [ 204.  204.  102.]
 [ 102.  102.   51.]]

## U
[[ -7.00140042e-02  -9.72591516e-01   2.71726403e-02   6.77453453e-02
   -2.26865888e-03  -7.22826631e-02  -1.42296667e-01   1.35490691e-01]
 [ -1.40028008e-01   4.42198277e-02   9.88740330e-01   8.86553423e-03
   -2.96889371e-04  -9.45931298e-03  -1.86217366e-02   1.77310685e-02]
 [ -2.10042013e-01  -1.94195510e-02  -2.45201265e-02  -3.34857011e-01
   -3.48600646e-01  -3.62344281e-01  -3.76087917e-01  -6.69714021e-01]
 [ -2.80056017e-01   8.84396554e-02  -5.25962165e-02   9.14490775e-01
   -1.03834072e-01  -1.22158919e-01  -1.40483766e-01  -1.71018449e-01]
 [ -3.50070021e-01   2.48002767e-02  -5.08182347e-02  -8.10764574e-02
    8.96017484e-01  -1.26888575e-01  -1.49794634e-01  -1.62152915e-01]
 [ -4.20084025e-01  -3.88391021e-02  -4.90402530e-02  -7.66436903e-02
   -1.04130961e-01   8.68381768e-01  -1.59105503e-01  -1.53287381e-01]
 [ -4.90098029e-01  -1.02478481e-01  -4.72622713e-02  -7.22109232e-02
   -1.04279406e-01  -1.36347888e-01   8.31583629e-01  -1.44421846e-01]
 [ -5.60112034e-01   1.76879311e-01  -1.05192433e-01  -1.71018449e-01
   -2.07668143e-01  -2.44317838e-01  -2.80967532e-01   6.57963102e-01]]

## Sigma
[  2.14242853e+01   1.93007163e-15   2.53327545e-31]

## V_tranpose, othornormal 
[[-0.66666667 -0.66666667 -0.33333333]
 [ 0.74535599 -0.59628479 -0.2981424 ]
 [-0.          0.4472136  -0.89442719]]

```
[-0.66666667 -0.66666667 -0.33333333] is the new angle that provides most variance. One interesting thing to notice is that (x,y) **co-varies** 2 times than (y,z) or (x,z) in covariance matrix, which coincides with the 1st new angle in SVD.

Intuitively, we have already computed the angle with the most variance when computing covariance matrix in 1st step of SVD.


**SVD from scratch: Power iteration to compute**


$$ (U\Sigma V^T)^T $$





\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)