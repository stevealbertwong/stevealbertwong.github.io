---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---


**It sounds kind of impossible**

## PCA's objective corresponds with SVD

Given any matrix \\(A\\), it is factorized into \\(U \Sigma V^T \\) 

$$ A = U \Sigma V^T $$



This is a good [SVD visualization](http://setosa.io/ev/principal-component-analysis/).

<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis



<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>
</div>

> Assumes blue array is x-axis, red y-axis, green z-axis.

Here is some fake data from that represents that graph's dataset. It is clear that there is a almost upward trend of the data.



```python
import numpy as np

# each row represents 1 data point, with 3 attributes x,y,z
dataset = [
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
]
```


The real question PCA is asking is if we have to cut 1 axis, which one should we cut that minimize the lost of information. The answer might not be that clear in before change of basis. But if we change the basis by rotating the coordinate system, blue arguably should be the one gets cut as it has the least variance within the new coordinate system.(notice blue is no longer x-axis)



**Covariance matrix**

Geometrically, SVD could be think of a rotation, stretch and rotation. 

$$ A = U \Sigma V^T $$

$$ U^T $$




Assuming matrix \\( A \\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. \\(A^T A\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.



$$ 
\begin{align}
A = U \Sigma V^T 
\end{align}
$$

$$ A^T A = {(U\Sigma V^T)}^T (U \Sigma V^T) $$

$$ A^T A = {(V\Sigma U^T)} (U \Sigma V^T) $$

$$ A^T A = V {\Sigma}^2 V^T $$ 

note: since orthonormal eigenvectors transpose mulitply by itself results in identity matrix


\\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\SigmaV^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.



**Power iteration to compute**


$$ (U\Sigma V^T)^T $$





\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)