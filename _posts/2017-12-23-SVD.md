---
layout: post
comments: true
title:  "Singular Value Decomposition from scratch"
excerpt: "1. Geometric meaning and intuition 2. Power iteration to compute "
date:   2017-12-23 11:00:00
mathjax: true
---



** SVD objective **

Given any matrix \\(A\\), it is factorized into \\(U \Sigma V^T \\) 

$$ A = U \Sigma V^T $$


<div class="imgcap">
<img src="/assets/svd/before_change_of_basis.png" height="200">
<div class="thecap">Before change of basis

<img src="/assets/svd/after_change_of_basis.png" height="200">
<div class="thecap">After change of basis
</div>
</div>


```python

# each row represents 1 data point, with 3 attributes x,y,z
dataset = [
	[1,1,0.5],
	[2,2,1],
	[3,3,1.5],
	[4,4,2],
	[5,5,2.5],
	[6,6,3],
	[7,7,3.5],
	[8,8,4]
]


```



** Change of basis and Covariance matrix **

Geometrically, SVD could be think of a rotation, stretch and rotation. 

$$ A = U \Sigma V^T $$
$$ U^T $$



Assuming matrix \\(A\\) represents the dataset where rows denotes sample index and columns denotes attributes of each sample. \\(A^T A\\) could be thought of computing covariance matrix between attributes. Each entry in covariance matrix represents attributes similarity between 2 attributes.

$$ A = U \Sigma V^T $$
$$ A^T A = {(U\Sigma V^T)}^T (U \Sigma V^T) $$

$$ A^T A = {(V\Sigma U^T)} (U \Sigma V^T) $$
since orthonormal eigenvectors transpose mulitply by itself results in identity matrix
$$ A^T A = V {\Sigma}^2 V^T $$ 

\\(V\\) is change of basis matrix and \\(U\\) is data represented in that basis.

If you prefer change of basis or transformation matrix at the front and magnitude vector at the end, you could think as:

$$ U\SigmaV^T = {V^T \Sigma U}^T $$

this just requires you to transpose the dataset matrix first.





$$ (U\Sigma V^T)^T $$




** Covariance matrix **

\\( XV = U\Sigma \\)

\\( \sum\_i \log p(y\_i \mid x\_i) \\)