# Bayes' rule in inference

## Amazon Rain Forest
Determining probability of a particular tribe having disease when you only see 3 people not having disease.

If he comes from a tribe that does not have disease => probability of him having disease = 0
else if he comes from tribe that has disease => probability = 0.5

IID: same tribe but not same family

$\theta$ = 1 means have disease

Factors affecting your judgment on probability:
1. probability of getting infected by the disease
2. probability of 

$$ \begin{align} \nabla_{\theta} E_x[f(x)] &= \nabla_{\theta} \sum_x p(x) f(x) & \text{definition of expectation} \ & = \sum_x \nabla_{\theta} p(x) f(x) & \text{swap sum and gradient} \ & = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) & \text{both multiply and divide by } p(x) \ & = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) & \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \ & = E_x[f(x) \nabla_{\theta} \log p(x) ] & \text{definition of expectation} \end{align} $$


## Posterior

We are interested to learn probability of which tribe the 3 people come from given they all have no disease.

$P(\theta =1|data, model)$
i.e. given what we observe and what we choose as model

## Likelihood

Probability of seeing data given such parameter under such model.

Probability of coming from a tribe that has a disease, given 3 of them all not have disease.

$$P(data|\theta, model)$$(1)

When you see the data, you assume you did not see every part of the data. You assume there are parameters $\theta$ and some model that controls probability of data.

Objective in the sense it comes from past history statistics where data is more or less likely to see under certain $\theta$

## Prior

How rare is the parameter under model

Prior denotes your previous belief about $\theta$. 
It is your subjective choice to manipulate the model despite the data seen. i.e. Despite seeing certain data under certain $\theta$, you could decide as prior the probability of seeing such $\theta$ is slim and so posterior of such prior is still slim.

Common choices include uniform, normal.

## Denominator


$$P(data|Model) = P(data|\theta,Model) * P(\theta|Model) & \text{Chain rule of conditional probability to integrate out}\theta$$

https://en.wikipedia.org/wiki/Chain_rule_(probability)

$P(A_4,A_3,A_2,A_1) = P(A_1)*P(A_2|A_1)*P(A_3|A_2,A_1)*P(A_4|A_3,A_2,A_1)$

Think of example of P(rain, umbrella, raincoats) as doing elimination in tables with columes equal to $A_4, A_3, A_2, A_1$


Integrating out raincoats means ignoring the requirement that raincoats have to be 1 in rows that already contains raincoats and umbrella being 1. $P(Raincoats|Umbrella)$ should be the answer since it looks at rows only when umbrellas are 1 and multiply by proportion of Raincoats being any value.


Think of this world as having a giant table containing columns of model, parameters and data with rows denoting all of their possible values.

?? prior conjugate
?? complete conditional + exponential family
gibbs samplier => sampling complete conditional



reference: 
https://www.youtube.com/watch?v=a5QDDZLGSXY&list=PLFDbGp5YzjqXQ4oE4w9GVWdiokWB9gEpm&index=17

5-10