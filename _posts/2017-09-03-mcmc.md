## What is MCMC?

## Why MCMC?

## Transition Matrix, Stationary Distribution, Detailed Balance Condition

```python

```

Given posterior as stationary distribution of a Markov Chain, we generate samples from this posterior distribution. (TBC)


Given the stationary distribution of our Markov Chain, the posterior integral from Bayesian inference is intractable. (TBC) => but how is MCMC linked to transition matrix and posterior bayesian inference ??

## Intuition from Reject Sampling

Reject Sampling is an important building block in MCMC.

Reject Sampling provides an easy way to sample form a complex distribution whose integration is not immediately obvious.

The only requirement is to define a wrapper probability density function that wraps the real data distribution we try to approximate. It does not matter what the distribution of wrapper is, it could be uniform or normal or any other distribution.

Let's assume we use a normal as wrapper.
The intuition of approximation is:
* acceptance probability is the ratio between the height of wrapper and real data approximation, where height of wrapper represents the times we will draw such value and height of real data approximation represents the volume/amount of such real data's value
* when we draw from wrapper function from a normal distribution, value around the mean $\mu$ should have a higher presence (we could see this from the height of wrapper) i.e. we are likely to draw more of value around the mean $\mu$
* however, the uniform distribution acted as a gate/threshold to proportionally allow value to come through where higher value has a proportionally higher chance to be "accepted" as an sample
* real data is approximated through the percentage the height ratio is making through the acceptance gate, when drawing in normal distribution
* Think of it as 3 scenarios when draws more: 1. draws more samples + acceptance ratio high = more accepted samples 2.draws more samples + super low acceptance ratio = very low samples (despite more draws, more proportion did not make through) 3. normal in wrapper + flat line in real data = it is true that we draw more samples, but acceptance ratio is decreased so real data approximation line remains flat

Talk is cheap. Show me the code!

```python
import scipy.stats as stat

# Plot between -10 and 10 with .001 steps.
x_axis = np.arange(-15, 15, 0.001)
target_func = stat.norm.pdf(x_axis,3,1) + stat.norm.pdf(x_axis, -5, 2)
plt.plot(x_axis, target_func, color = 'green', label = 'target')

# wrapper function
wrapper_func = stat.norm.pdf(x_axis,0,4)
c =  np.max(target_func/wrapper_func)

#PLOT SCALED PROPOSAL/ENVELOP DISTRIBUTION
plt.plot(x_axis,c*wrapper_func, color = 'pink', label = 'wrapper');
plt.legend()
plt.show()

#probability of getting X at the target function;
def target_p(x):
    p = stat.norm.pdf(x,3,1) + stat.norm.pdf(x, -5, 2);
    return p;

#perform reject_sampling to get samples at size sampling_nums
def reject_sampling( sampling_nums):
    samples = []
    sample_num = 0;
    while(sample_num < sampling_nums):
        #sampling from wrapper function
        X_i = np.random.normal(0,4,1)[0];
        #calculate acceptance probability
            # q(X_i) is the probability of X_i given wrapper function; where loc = mean, scale = sigma (which isstdev, sqrt (4))
        a = target_p(X_i)/ (c*(stat.norm.pdf(X_i, loc=0, scale=2)));
        #compare with random value from Uniform 
        u = np.random.uniform(0,1,1);
        # if acceptance rate bigger than a random probability value, we accept X_i; vice versa
        if a>= u:
            samples.append(X_i);
            sample_num += 1;
        else:
            pass;
    return samples;

samples = reject_sampling(10000);

#plot the shistogram
plt.hist(samples, bins = 500, color = 'gray');
plt.show();

```
